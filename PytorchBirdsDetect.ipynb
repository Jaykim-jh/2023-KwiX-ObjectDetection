{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "\n",
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "\n",
    "# dataset and transformation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import os\n",
    "\n",
    "# display images\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=\"C:\\Download/Birds525/train\"\n",
    "test_ds=\"C:\\Download/Birds525/test\"\n",
    "validation_ds=\"C:\\Download/Birds525/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255) #initialize train generator \n",
    "valid_datagen = ImageDataGenerator(rescale = 1.0/255.) #initialize validation generator \n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.) #initialize test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84635 images belonging to 525 classes.\n",
      "Found 2625 images belonging to 525 classes.\n",
      "Found 2625 images belonging to 525 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15)\n",
    "test_datagen = ImageDataGenerator()\n",
    "valid_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_ds,target_size=(224, 224),batch_size=32,shuffle=True,class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory(test_ds,target_size=(224,224),batch_size=32,shuffle=False,class_mode='categorical')\n",
    "validation_generator = valid_datagen.flow_from_directory(validation_ds,target_size=(224,224),batch_size=32,shuffle=False,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645\n",
      "83\n"
     ]
    }
   ],
   "source": [
    "print(len(train_generator))\n",
    "print(len(validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformation\n",
    "transformation = transforms.Compose([transforms.ToTensor(), transforms.Resize(224)])\n",
    "\n",
    "# apply transformation to dataset\n",
    "train_generator.transform = transformation\n",
    "validation_generator.transform = transformation\n",
    "\n",
    "# make dataloade\n",
    "train_dl = DataLoader(train_generator, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(validation_generator, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor or list of tensors expected, got <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m x_grid \u001b[39m=\u001b[39m [train_generator[i][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rnd_ind]\n\u001b[0;32m     17\u001b[0m y_grid \u001b[39m=\u001b[39m [train_generator[i][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m rnd_ind]\n\u001b[1;32m---> 19\u001b[0m x_grid \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mmake_grid(x_grid, nrow\u001b[39m=\u001b[39;49mgrid_size, padding\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     20\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[0;32m     21\u001b[0m show(x_grid, y_grid)\n",
      "File \u001b[1;32mc:\\Users\\woghk\\anaconda3\\envs\\KwixTF\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\woghk\\anaconda3\\envs\\KwixTF\\lib\\site-packages\\torchvision\\utils.py:60\u001b[0m, in \u001b[0;36mmake_grid\u001b[1;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     _log_api_usage_once(make_grid)\n\u001b[0;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (torch\u001b[39m.\u001b[39mis_tensor(tensor) \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(tensor, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(torch\u001b[39m.\u001b[39mis_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tensor))):\n\u001b[1;32m---> 60\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensor or list of tensors expected, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrange\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     63\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m     64\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe parameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrange\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is deprecated since 0.12 and will be removed in 0.14. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease use \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue_range\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: tensor or list of tensors expected, got <class 'list'>"
     ]
    }
   ],
   "source": [
    "# check sample images\n",
    "def show(img, y=None):\n",
    "    npimg = img.numpy()\n",
    "    npimg_tr = np.transpose(npimg, (1, 2, 0))\n",
    "    plt.imshow(npimg_tr)\n",
    "\n",
    "    if y is not None:\n",
    "        plt.title('labels:' + str(y))\n",
    "\n",
    "np.random.seed(10)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "grid_size=4\n",
    "rnd_ind = np.random.randint(0, len(train_generator), grid_size)\n",
    "\n",
    "x_grid = [train_generator[i][0] for i in rnd_ind]\n",
    "y_grid = [train_generator[i][1] for i in rnd_ind]\n",
    "\n",
    "x_grid = utils.make_grid(x_grid, nrow=grid_size, padding=2)\n",
    "plt.figure(figsize=(10,10))\n",
    "show(x_grid, y_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Swish activation function\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(3, 3, 224, 224)\n",
    "    model = Swish()\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 56, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# SE Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, r=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels * r),\n",
    "            Swish(),\n",
    "            nn.Linear(in_channels * r, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.excitation(x)\n",
    "        x = x.view(x.size(0), x.size(1), 1, 1)\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(3, 56, 17, 17)\n",
    "    model = SEBlock(x.size(1))\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 16, 24, 24]) Stochastic depth: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "class MBConv(nn.Module):\n",
    "    expand = 6\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first MBConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * MBConv.expand, 1, stride=stride, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_channels * MBConv.expand, in_channels * MBConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*MBConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * MBConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*MBConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(3, 16, 24, 24)\n",
    "    model = MBConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "    model.train()\n",
    "    output = model(x)\n",
    "    x = (output == x)\n",
    "    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 16, 24, 24]) Stochastic depth: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "class SepConv(nn.Module):\n",
    "    expand = 1\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n",
    "        super().__init__()\n",
    "        # first SepConv is not using stochastic depth\n",
    "        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n",
    "\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * SepConv.expand, in_channels * SepConv.expand, kernel_size=kernel_size,\n",
    "                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*SepConv.expand),\n",
    "            nn.BatchNorm2d(in_channels * SepConv.expand, momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.se = SEBlock(in_channels * SepConv.expand, se_scale)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels*SepConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.shortcut = (stride == 1) and (in_channels == out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # stochastic depth\n",
    "        if self.training:\n",
    "            if not torch.bernoulli(self.p):\n",
    "                return x\n",
    "\n",
    "        x_shortcut = x\n",
    "        x_residual = self.residual(x)\n",
    "        x_se = self.se(x_residual)\n",
    "\n",
    "        x = x_se * x_residual\n",
    "        x = self.project(x)\n",
    "\n",
    "        if self.shortcut:\n",
    "            x= x_shortcut + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(3, 16, 24, 24)\n",
    "    model = SepConv(x.size(1), x.size(1), 3, stride=1, p=1)\n",
    "    model.train()\n",
    "    output = model(x)\n",
    "    # stochastic depth check\n",
    "    x = (output == x)\n",
    "    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 525])\n"
     ]
    }
   ],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, width_coef=1., depth_coef=1., scale=1., dropout=0.2, se_scale=4, stochastic_depth=False, p=0.5):\n",
    "        super().__init__()\n",
    "        channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
    "        repeats = [1, 2, 2, 3, 3, 4, 1]\n",
    "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
    "        kernel_size = [3, 3, 5, 3, 5, 5, 3]\n",
    "        depth = depth_coef\n",
    "        width = width_coef\n",
    "\n",
    "        channels = [int(x*width) for x in channels]\n",
    "        repeats = [int(x*depth) for x in repeats]\n",
    "\n",
    "        # stochastic depth\n",
    "        if stochastic_depth:\n",
    "            self.p = p\n",
    "            self.step = (1 - 0.5) / (sum(repeats) - 1)\n",
    "        else:\n",
    "            self.p = 1\n",
    "            self.step = 0\n",
    "\n",
    "\n",
    "        # efficient net\n",
    "        self.upsample = nn.Upsample(scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(3, channels[0],3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[0], momentum=0.99, eps=1e-3)\n",
    "        )\n",
    "\n",
    "        self.stage2 = self._make_Block(SepConv, repeats[0], channels[0], channels[1], kernel_size[0], strides[0], se_scale)\n",
    "\n",
    "        self.stage3 = self._make_Block(MBConv, repeats[1], channels[1], channels[2], kernel_size[1], strides[1], se_scale)\n",
    "\n",
    "        self.stage4 = self._make_Block(MBConv, repeats[2], channels[2], channels[3], kernel_size[2], strides[2], se_scale)\n",
    "\n",
    "        self.stage5 = self._make_Block(MBConv, repeats[3], channels[3], channels[4], kernel_size[3], strides[3], se_scale)\n",
    "\n",
    "        self.stage6 = self._make_Block(MBConv, repeats[4], channels[4], channels[5], kernel_size[4], strides[4], se_scale)\n",
    "\n",
    "        self.stage7 = self._make_Block(MBConv, repeats[5], channels[5], channels[6], kernel_size[5], strides[5], se_scale)\n",
    "\n",
    "        self.stage8 = self._make_Block(MBConv, repeats[6], channels[6], channels[7], kernel_size[6], strides[6], se_scale)\n",
    "\n",
    "        self.stage9 = nn.Sequential(\n",
    "            nn.Conv2d(channels[7], channels[8], 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[8], momentum=0.99, eps=1e-3),\n",
    "            Swish()\n",
    "        ) \n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(channels[8], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        x = self.stage5(x)\n",
    "        x = self.stage6(x)\n",
    "        x = self.stage7(x)\n",
    "        x = self.stage8(x)\n",
    "        x = self.stage9(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, se_scale):\n",
    "        strides = [stride] + [1] * (repeats - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, kernel_size, stride, se_scale, self.p))\n",
    "            in_channels = out_channels\n",
    "            self.p -= self.step\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def efficientnet_b0(num_classes=525):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.0, scale=1.0,dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b1(num_classes=525):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.1, scale=240/224, dropout=0.2, se_scale=4)\n",
    "\n",
    "def efficientnet_b2(num_classes=525):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.1, depth_coef=1.2, scale=260/224., dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b3(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.2, depth_coef=1.4, scale=300/224, dropout=0.3, se_scale=4)\n",
    "\n",
    "def efficientnet_b4(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.4, depth_coef=1.8, scale=380/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b5(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.6, depth_coef=2.2, scale=456/224, dropout=0.4, se_scale=4)\n",
    "\n",
    "def efficientnet_b6(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=1.8, depth_coef=2.6, scale=528/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "def efficientnet_b7(num_classes=10):\n",
    "    return EfficientNet(num_classes=num_classes, width_coef=2.0, depth_coef=3.1, scale=600/224, dropout=0.5, se_scale=4)\n",
    "\n",
    "\n",
    "# check\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    x = torch.randn(3, 3, 224, 224).to(device)\n",
    "    model = efficientnet_b0().to(device)\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "          Upsample-1          [-1, 3, 224, 224]               0\n",
      "            Conv2d-2         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-3         [-1, 32, 112, 112]              64\n",
      "            Conv2d-4         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "           Sigmoid-6         [-1, 32, 112, 112]               0\n",
      "             Swish-7         [-1, 32, 112, 112]               0\n",
      " AdaptiveAvgPool2d-8             [-1, 32, 1, 1]               0\n",
      "            Linear-9                  [-1, 128]           4,224\n",
      "          Sigmoid-10                  [-1, 128]               0\n",
      "            Swish-11                  [-1, 128]               0\n",
      "           Linear-12                   [-1, 32]           4,128\n",
      "          Sigmoid-13                   [-1, 32]               0\n",
      "          SEBlock-14             [-1, 32, 1, 1]               0\n",
      "           Conv2d-15         [-1, 16, 112, 112]             512\n",
      "      BatchNorm2d-16         [-1, 16, 112, 112]              32\n",
      "          SepConv-17         [-1, 16, 112, 112]               0\n",
      "           Conv2d-18           [-1, 96, 56, 56]           1,536\n",
      "      BatchNorm2d-19           [-1, 96, 56, 56]             192\n",
      "          Sigmoid-20           [-1, 96, 56, 56]               0\n",
      "            Swish-21           [-1, 96, 56, 56]               0\n",
      "           Conv2d-22           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-23           [-1, 96, 56, 56]             192\n",
      "          Sigmoid-24           [-1, 96, 56, 56]               0\n",
      "            Swish-25           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-26             [-1, 96, 1, 1]               0\n",
      "           Linear-27                  [-1, 384]          37,248\n",
      "          Sigmoid-28                  [-1, 384]               0\n",
      "            Swish-29                  [-1, 384]               0\n",
      "           Linear-30                   [-1, 96]          36,960\n",
      "          Sigmoid-31                   [-1, 96]               0\n",
      "          SEBlock-32             [-1, 96, 1, 1]               0\n",
      "           Conv2d-33           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-34           [-1, 24, 56, 56]              48\n",
      "           MBConv-35           [-1, 24, 56, 56]               0\n",
      "           Conv2d-36          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-37          [-1, 144, 56, 56]             288\n",
      "          Sigmoid-38          [-1, 144, 56, 56]               0\n",
      "            Swish-39          [-1, 144, 56, 56]               0\n",
      "           Conv2d-40          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-41          [-1, 144, 56, 56]             288\n",
      "          Sigmoid-42          [-1, 144, 56, 56]               0\n",
      "            Swish-43          [-1, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-44            [-1, 144, 1, 1]               0\n",
      "           Linear-45                  [-1, 576]          83,520\n",
      "          Sigmoid-46                  [-1, 576]               0\n",
      "            Swish-47                  [-1, 576]               0\n",
      "           Linear-48                  [-1, 144]          83,088\n",
      "          Sigmoid-49                  [-1, 144]               0\n",
      "          SEBlock-50            [-1, 144, 1, 1]               0\n",
      "           Conv2d-51           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-52           [-1, 24, 56, 56]              48\n",
      "           MBConv-53           [-1, 24, 56, 56]               0\n",
      "           Conv2d-54          [-1, 144, 28, 28]           3,456\n",
      "      BatchNorm2d-55          [-1, 144, 28, 28]             288\n",
      "          Sigmoid-56          [-1, 144, 28, 28]               0\n",
      "            Swish-57          [-1, 144, 28, 28]               0\n",
      "           Conv2d-58          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-59          [-1, 144, 28, 28]             288\n",
      "          Sigmoid-60          [-1, 144, 28, 28]               0\n",
      "            Swish-61          [-1, 144, 28, 28]               0\n",
      "AdaptiveAvgPool2d-62            [-1, 144, 1, 1]               0\n",
      "           Linear-63                  [-1, 576]          83,520\n",
      "          Sigmoid-64                  [-1, 576]               0\n",
      "            Swish-65                  [-1, 576]               0\n",
      "           Linear-66                  [-1, 144]          83,088\n",
      "          Sigmoid-67                  [-1, 144]               0\n",
      "          SEBlock-68            [-1, 144, 1, 1]               0\n",
      "           Conv2d-69           [-1, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-70           [-1, 40, 28, 28]              80\n",
      "           MBConv-71           [-1, 40, 28, 28]               0\n",
      "           Conv2d-72          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-73          [-1, 240, 28, 28]             480\n",
      "          Sigmoid-74          [-1, 240, 28, 28]               0\n",
      "            Swish-75          [-1, 240, 28, 28]               0\n",
      "           Conv2d-76          [-1, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-77          [-1, 240, 28, 28]             480\n",
      "          Sigmoid-78          [-1, 240, 28, 28]               0\n",
      "            Swish-79          [-1, 240, 28, 28]               0\n",
      "AdaptiveAvgPool2d-80            [-1, 240, 1, 1]               0\n",
      "           Linear-81                  [-1, 960]         231,360\n",
      "          Sigmoid-82                  [-1, 960]               0\n",
      "            Swish-83                  [-1, 960]               0\n",
      "           Linear-84                  [-1, 240]         230,640\n",
      "          Sigmoid-85                  [-1, 240]               0\n",
      "          SEBlock-86            [-1, 240, 1, 1]               0\n",
      "           Conv2d-87           [-1, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-88           [-1, 40, 28, 28]              80\n",
      "           MBConv-89           [-1, 40, 28, 28]               0\n",
      "           Conv2d-90          [-1, 240, 14, 14]           9,600\n",
      "      BatchNorm2d-91          [-1, 240, 14, 14]             480\n",
      "          Sigmoid-92          [-1, 240, 14, 14]               0\n",
      "            Swish-93          [-1, 240, 14, 14]               0\n",
      "           Conv2d-94          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-95          [-1, 240, 14, 14]             480\n",
      "          Sigmoid-96          [-1, 240, 14, 14]               0\n",
      "            Swish-97          [-1, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-98            [-1, 240, 1, 1]               0\n",
      "           Linear-99                  [-1, 960]         231,360\n",
      "         Sigmoid-100                  [-1, 960]               0\n",
      "           Swish-101                  [-1, 960]               0\n",
      "          Linear-102                  [-1, 240]         230,640\n",
      "         Sigmoid-103                  [-1, 240]               0\n",
      "         SEBlock-104            [-1, 240, 1, 1]               0\n",
      "          Conv2d-105           [-1, 80, 14, 14]          19,200\n",
      "     BatchNorm2d-106           [-1, 80, 14, 14]             160\n",
      "          MBConv-107           [-1, 80, 14, 14]               0\n",
      "          Conv2d-108          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-109          [-1, 480, 14, 14]             960\n",
      "         Sigmoid-110          [-1, 480, 14, 14]               0\n",
      "           Swish-111          [-1, 480, 14, 14]               0\n",
      "          Conv2d-112          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-113          [-1, 480, 14, 14]             960\n",
      "         Sigmoid-114          [-1, 480, 14, 14]               0\n",
      "           Swish-115          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-116            [-1, 480, 1, 1]               0\n",
      "          Linear-117                 [-1, 1920]         923,520\n",
      "         Sigmoid-118                 [-1, 1920]               0\n",
      "           Swish-119                 [-1, 1920]               0\n",
      "          Linear-120                  [-1, 480]         922,080\n",
      "         Sigmoid-121                  [-1, 480]               0\n",
      "         SEBlock-122            [-1, 480, 1, 1]               0\n",
      "          Conv2d-123           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-124           [-1, 80, 14, 14]             160\n",
      "          MBConv-125           [-1, 80, 14, 14]               0\n",
      "          Conv2d-126          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-127          [-1, 480, 14, 14]             960\n",
      "         Sigmoid-128          [-1, 480, 14, 14]               0\n",
      "           Swish-129          [-1, 480, 14, 14]               0\n",
      "          Conv2d-130          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-131          [-1, 480, 14, 14]             960\n",
      "         Sigmoid-132          [-1, 480, 14, 14]               0\n",
      "           Swish-133          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-134            [-1, 480, 1, 1]               0\n",
      "          Linear-135                 [-1, 1920]         923,520\n",
      "         Sigmoid-136                 [-1, 1920]               0\n",
      "           Swish-137                 [-1, 1920]               0\n",
      "          Linear-138                  [-1, 480]         922,080\n",
      "         Sigmoid-139                  [-1, 480]               0\n",
      "         SEBlock-140            [-1, 480, 1, 1]               0\n",
      "          Conv2d-141           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-142           [-1, 80, 14, 14]             160\n",
      "          MBConv-143           [-1, 80, 14, 14]               0\n",
      "          Conv2d-144          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-145          [-1, 480, 14, 14]             960\n",
      "         Sigmoid-146          [-1, 480, 14, 14]               0\n",
      "           Swish-147          [-1, 480, 14, 14]               0\n",
      "          Conv2d-148          [-1, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-149          [-1, 480, 14, 14]             960\n",
      "         Sigmoid-150          [-1, 480, 14, 14]               0\n",
      "           Swish-151          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-152            [-1, 480, 1, 1]               0\n",
      "          Linear-153                 [-1, 1920]         923,520\n",
      "         Sigmoid-154                 [-1, 1920]               0\n",
      "           Swish-155                 [-1, 1920]               0\n",
      "          Linear-156                  [-1, 480]         922,080\n",
      "         Sigmoid-157                  [-1, 480]               0\n",
      "         SEBlock-158            [-1, 480, 1, 1]               0\n",
      "          Conv2d-159          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-160          [-1, 112, 14, 14]             224\n",
      "          MBConv-161          [-1, 112, 14, 14]               0\n",
      "          Conv2d-162          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-163          [-1, 672, 14, 14]           1,344\n",
      "         Sigmoid-164          [-1, 672, 14, 14]               0\n",
      "           Swish-165          [-1, 672, 14, 14]               0\n",
      "          Conv2d-166          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-167          [-1, 672, 14, 14]           1,344\n",
      "         Sigmoid-168          [-1, 672, 14, 14]               0\n",
      "           Swish-169          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-170            [-1, 672, 1, 1]               0\n",
      "          Linear-171                 [-1, 2688]       1,809,024\n",
      "         Sigmoid-172                 [-1, 2688]               0\n",
      "           Swish-173                 [-1, 2688]               0\n",
      "          Linear-174                  [-1, 672]       1,807,008\n",
      "         Sigmoid-175                  [-1, 672]               0\n",
      "         SEBlock-176            [-1, 672, 1, 1]               0\n",
      "          Conv2d-177          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-178          [-1, 112, 14, 14]             224\n",
      "          MBConv-179          [-1, 112, 14, 14]               0\n",
      "          Conv2d-180          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-181          [-1, 672, 14, 14]           1,344\n",
      "         Sigmoid-182          [-1, 672, 14, 14]               0\n",
      "           Swish-183          [-1, 672, 14, 14]               0\n",
      "          Conv2d-184          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-185          [-1, 672, 14, 14]           1,344\n",
      "         Sigmoid-186          [-1, 672, 14, 14]               0\n",
      "           Swish-187          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-188            [-1, 672, 1, 1]               0\n",
      "          Linear-189                 [-1, 2688]       1,809,024\n",
      "         Sigmoid-190                 [-1, 2688]               0\n",
      "           Swish-191                 [-1, 2688]               0\n",
      "          Linear-192                  [-1, 672]       1,807,008\n",
      "         Sigmoid-193                  [-1, 672]               0\n",
      "         SEBlock-194            [-1, 672, 1, 1]               0\n",
      "          Conv2d-195          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-196          [-1, 112, 14, 14]             224\n",
      "          MBConv-197          [-1, 112, 14, 14]               0\n",
      "          Conv2d-198            [-1, 672, 7, 7]          75,264\n",
      "     BatchNorm2d-199            [-1, 672, 7, 7]           1,344\n",
      "         Sigmoid-200            [-1, 672, 7, 7]               0\n",
      "           Swish-201            [-1, 672, 7, 7]               0\n",
      "          Conv2d-202            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-203            [-1, 672, 7, 7]           1,344\n",
      "         Sigmoid-204            [-1, 672, 7, 7]               0\n",
      "           Swish-205            [-1, 672, 7, 7]               0\n",
      "AdaptiveAvgPool2d-206            [-1, 672, 1, 1]               0\n",
      "          Linear-207                 [-1, 2688]       1,809,024\n",
      "         Sigmoid-208                 [-1, 2688]               0\n",
      "           Swish-209                 [-1, 2688]               0\n",
      "          Linear-210                  [-1, 672]       1,807,008\n",
      "         Sigmoid-211                  [-1, 672]               0\n",
      "         SEBlock-212            [-1, 672, 1, 1]               0\n",
      "          Conv2d-213            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-214            [-1, 192, 7, 7]             384\n",
      "          MBConv-215            [-1, 192, 7, 7]               0\n",
      "          Conv2d-216           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-217           [-1, 1152, 7, 7]           2,304\n",
      "         Sigmoid-218           [-1, 1152, 7, 7]               0\n",
      "           Swish-219           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-220           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-221           [-1, 1152, 7, 7]           2,304\n",
      "         Sigmoid-222           [-1, 1152, 7, 7]               0\n",
      "           Swish-223           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-224           [-1, 1152, 1, 1]               0\n",
      "          Linear-225                 [-1, 4608]       5,313,024\n",
      "         Sigmoid-226                 [-1, 4608]               0\n",
      "           Swish-227                 [-1, 4608]               0\n",
      "          Linear-228                 [-1, 1152]       5,309,568\n",
      "         Sigmoid-229                 [-1, 1152]               0\n",
      "         SEBlock-230           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-231            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-232            [-1, 192, 7, 7]             384\n",
      "          MBConv-233            [-1, 192, 7, 7]               0\n",
      "          Conv2d-234           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-235           [-1, 1152, 7, 7]           2,304\n",
      "         Sigmoid-236           [-1, 1152, 7, 7]               0\n",
      "           Swish-237           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-238           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-239           [-1, 1152, 7, 7]           2,304\n",
      "         Sigmoid-240           [-1, 1152, 7, 7]               0\n",
      "           Swish-241           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-242           [-1, 1152, 1, 1]               0\n",
      "          Linear-243                 [-1, 4608]       5,313,024\n",
      "         Sigmoid-244                 [-1, 4608]               0\n",
      "           Swish-245                 [-1, 4608]               0\n",
      "          Linear-246                 [-1, 1152]       5,309,568\n",
      "         Sigmoid-247                 [-1, 1152]               0\n",
      "         SEBlock-248           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-249            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-250            [-1, 192, 7, 7]             384\n",
      "          MBConv-251            [-1, 192, 7, 7]               0\n",
      "          Conv2d-252           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-253           [-1, 1152, 7, 7]           2,304\n",
      "         Sigmoid-254           [-1, 1152, 7, 7]               0\n",
      "           Swish-255           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-256           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-257           [-1, 1152, 7, 7]           2,304\n",
      "         Sigmoid-258           [-1, 1152, 7, 7]               0\n",
      "           Swish-259           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-260           [-1, 1152, 1, 1]               0\n",
      "          Linear-261                 [-1, 4608]       5,313,024\n",
      "         Sigmoid-262                 [-1, 4608]               0\n",
      "           Swish-263                 [-1, 4608]               0\n",
      "          Linear-264                 [-1, 1152]       5,309,568\n",
      "         Sigmoid-265                 [-1, 1152]               0\n",
      "         SEBlock-266           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-267            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-268            [-1, 192, 7, 7]             384\n",
      "          MBConv-269            [-1, 192, 7, 7]               0\n",
      "          Conv2d-270           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-271           [-1, 1152, 7, 7]           2,304\n",
      "         Sigmoid-272           [-1, 1152, 7, 7]               0\n",
      "           Swish-273           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-274           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-275           [-1, 1152, 7, 7]           2,304\n",
      "         Sigmoid-276           [-1, 1152, 7, 7]               0\n",
      "           Swish-277           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-278           [-1, 1152, 1, 1]               0\n",
      "          Linear-279                 [-1, 4608]       5,313,024\n",
      "         Sigmoid-280                 [-1, 4608]               0\n",
      "           Swish-281                 [-1, 4608]               0\n",
      "          Linear-282                 [-1, 1152]       5,309,568\n",
      "         Sigmoid-283                 [-1, 1152]               0\n",
      "         SEBlock-284           [-1, 1152, 1, 1]               0\n",
      "          Conv2d-285            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-286            [-1, 320, 7, 7]             640\n",
      "          MBConv-287            [-1, 320, 7, 7]               0\n",
      "          Conv2d-288           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-289           [-1, 1280, 7, 7]           2,560\n",
      "         Sigmoid-290           [-1, 1280, 7, 7]               0\n",
      "           Swish-291           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-292           [-1, 1280, 1, 1]               0\n",
      "         Dropout-293                 [-1, 1280]               0\n",
      "          Linear-294                  [-1, 525]         672,525\n",
      "================================================================\n",
      "Total params: 64,258,573\n",
      "Trainable params: 64,258,573\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 152.11\n",
      "Params size (MB): 245.13\n",
      "Estimated Total Size (MB): 397.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "model = efficientnet_b0().to(device)\n",
    "summary(model, (3,224,224), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function, optimizer, lr_scheduler\n",
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "\n",
    "# get current lr\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# calculate the metric per mini-batch\n",
    "def metric_batch(output, target):\n",
    "    pred = output.argmax(1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "# calculate the loss per mini-batch\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss_b = loss_func(output, target)\n",
    "    metric_b = metric_batch(output, target)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss_b.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    return loss_b.item(), metric_b\n",
    "\n",
    "\n",
    "# calculate the loss per epochs\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output = model(xb)\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "\n",
    "        running_loss += loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    metric = running_metric / len_data\n",
    "    return loss, metric\n",
    "\n",
    "\n",
    "# function to start training\n",
    "def train_val(model, params):\n",
    "    num_epochs=params['num_epochs']\n",
    "    loss_func=params['loss_func']\n",
    "    opt=params['optimizer']\n",
    "    train_dl=params['train_dl']\n",
    "    val_dl=params['val_dl']\n",
    "    sanity_check=params['sanity_check']\n",
    "    lr_scheduler=params['lr_scheduler']\n",
    "    path2weights=params['path2weights']\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    metric_history = {'train': [], 'val': []}\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print('Copied best model weights!')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print('Loading best model weights!')\n",
    "            model.load_state_dict(best_model_wts)\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training parameters\n",
    "params_train = {\n",
    "    'num_epochs':100,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':train_dl,\n",
    "    'val_dl':val_dl,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights':'./models/weights.pt',\n",
    "}\n",
    "\n",
    "# check the directory to save weights.pt\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "createFolder('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99, current lr= 0.01\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Got 5D input, but bilinear mode needs 4D input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, loss_hist, metric_hist \u001b[39m=\u001b[39m train_val(model, params_train)\n",
      "Cell \u001b[1;32mIn[74], line 84\u001b[0m, in \u001b[0;36mtrain_val\u001b[1;34m(model, params)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, current lr= \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch, num_epochs\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, current_lr))\n\u001b[0;32m     83\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 84\u001b[0m train_loss, train_metric \u001b[39m=\u001b[39m loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n\u001b[0;32m     85\u001b[0m loss_history[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     86\u001b[0m metric_history[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_metric)\n",
      "Cell \u001b[1;32mIn[74], line 44\u001b[0m, in \u001b[0;36mloss_epoch\u001b[1;34m(model, loss_func, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[0;32m     42\u001b[0m xb \u001b[39m=\u001b[39m xb\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     43\u001b[0m yb \u001b[39m=\u001b[39m yb\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 44\u001b[0m output \u001b[39m=\u001b[39m model(xb)\n\u001b[0;32m     46\u001b[0m loss_b, metric_b \u001b[39m=\u001b[39m loss_batch(loss_func, output, yb, opt)\n\u001b[0;32m     48\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_b\n",
      "File \u001b[1;32mc:\\Users\\woghk\\anaconda3\\envs\\KwixTF\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[72], line 56\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 56\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupsample(x)\n\u001b[0;32m     57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstage1(x)\n\u001b[0;32m     58\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstage2(x)\n",
      "File \u001b[1;32mc:\\Users\\woghk\\anaconda3\\envs\\KwixTF\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\woghk\\anaconda3\\envs\\KwixTF\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:153\u001b[0m, in \u001b[0;36mUpsample.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49minterpolate(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_factor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malign_corners,\n\u001b[0;32m    154\u001b[0m                          recompute_scale_factor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecompute_scale_factor)\n",
      "File \u001b[1;32mc:\\Users\\woghk\\anaconda3\\envs\\KwixTF\\lib\\site-packages\\torch\\nn\\functional.py:3940\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGot 5D input, but linear mode needs 3D input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3939\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 3940\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGot 5D input, but bilinear mode needs 4D input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3942\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   3943\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mInput Error: Only 3D, 4D and 5D input Tensors supported\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3944\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD) for the modes: nearest | linear | bilinear | bicubic | trilinear | area | nearest-exact\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3945\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), mode)\n\u001b[0;32m   3946\u001b[0m )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Got 5D input, but bilinear mode needs 4D input"
     ]
    }
   ],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Plot train-val loss\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mTrain-Val Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), loss_hist[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mplot(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), loss_hist[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_hist' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = params_train['num_epochs']\n",
    "\n",
    "# Plot train-val loss\n",
    "plt.title('Train-Val Loss')\n",
    "plt.plot(range(1, num_epochs+1), loss_hist['train'], label='train')\n",
    "plt.plot(range(1, num_epochs+1), loss_hist['val'], label='val')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot train-val accuracy\n",
    "plt.title('Train-Val Accuracy')\n",
    "plt.plot(range(1, num_epochs+1), metric_hist['train'], label='train')\n",
    "plt.plot(range(1, num_epochs+1), metric_hist['val'], label='val')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "use_gpu",
   "language": "python",
   "name": "kwixtf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
